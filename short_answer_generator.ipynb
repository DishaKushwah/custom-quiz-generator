{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7g0SJRLLsLDtjwp9pGEPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DishaKushwah/custom-quiz-generator/blob/main/short_answer_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8kqiMR4-nfk",
        "outputId": "e957a36d-11d4-4bca-8e8c-3e083923a53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n"
          ]
        }
      ],
      "source": [
        "## SHORT ANSWERS\n",
        "%pip install --upgrade transformers\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering,\n",
        "    pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
        ")\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ShortAnswerQuestion:\n",
        "    \"\"\"Data class for short answer questions.\"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    context_sentence: str\n",
        "    question_type: str\n",
        "    difficulty: str\n",
        "    confidence: float\n",
        "    keywords: List[str]\n",
        "    expected_length: str\n",
        "\n",
        "class AdvancedShortAnswerGenerator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize with state-of-the-art models for question generation.\"\"\"\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Load the best question generation model - T5-large fine-tuned for QG\n",
        "        self.qg_model_name = \"valhalla/t5-base-qg-hl\"\n",
        "        # Use AutoTokenizer and AutoModelForSeq2SeqLM for broader compatibility\n",
        "        self.qg_tokenizer = AutoTokenizer.from_pretrained(self.qg_model_name)\n",
        "        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(self.qg_model_name).to(self.device)\n",
        "\n",
        "        # Load FLAN-T5 for better question generation\n",
        "        self.flan_model_name = \"google/flan-t5-base\"\n",
        "        self.flan_tokenizer = AutoTokenizer.from_pretrained(self.flan_model_name)\n",
        "        self.flan_model = AutoModelForSeq2SeqLM.from_pretrained(self.flan_model_name).to(self.device)\n",
        "\n",
        "\n",
        "        # Load DeBERta for high-quality answer extraction\n",
        "        self.qa_model_name = \"deepset/roberta-base-squad2\"\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(self.qa_model_name)\n",
        "        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(self.qa_model_name).to(self.device)\n",
        "\n",
        "        # Load sentence transformer for semantic analysis\n",
        "        print(\"Loading sentence transformer...\")\n",
        "        self.sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "        # Load spaCy for advanced NLP\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = None\n",
        "\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            nltk.download('stopwords', quiet=True)\n",
        "            nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "            nltk.download('punkt_tab', quiet=True) # Added download for punkt_tab\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Question type templates (can be used as fallback or for prompt engineering)\n",
        "        self.question_templates = {\n",
        "            'factual': [\n",
        "                \"What is {}?\",\n",
        "                \"What does {} mean?\",\n",
        "                \"What are the characteristics of {}?\",\n",
        "                \"Define {}.\",\n",
        "                \"Explain {}.\"\n",
        "            ],\n",
        "            'analytical': [\n",
        "                \"How does {} work?\",\n",
        "                \"Why is {} important?\",\n",
        "                \"What is the significance of {}?\",\n",
        "                \"How does {} relate to {}?\",\n",
        "                \"What are the implications of {}?\"\n",
        "            ],\n",
        "            'comparative': [\n",
        "                \"Compare {} and {}.\",\n",
        "                \"What are the differences between {} and {}?\",\n",
        "                \"How does {} differ from {}?\",\n",
        "                \"What are the similarities between {} and {}?\"\n",
        "            ],\n",
        "            'causal': [\n",
        "                \"What caused {}?\",\n",
        "                \"What are the effects of {}?\",\n",
        "                \"How did {} lead to {}?\",\n",
        "                \"What resulted from {}?\"\n",
        "            ],\n",
        "            'procedural': [\n",
        "                \"How do you {}?\",\n",
        "                \"What are the steps to {}?\",\n",
        "                \"Describe the process of {}.\",\n",
        "                \"What is the procedure for {}?\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def extract_key_concepts(self, text: str) -> Dict:\n",
        "        \"\"\"Extract key concepts and entities from text.\"\"\"\n",
        "        if not self.nlp:\n",
        "            return {\"entities\": [], \"concepts\": [], \"sentences\": []}\n",
        "\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            # Include more entity types for broader question generation\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'CARDINAL', 'ORDINAL', 'NORP', 'LOC', 'PRODUCT']:\n",
        "                entities.append({\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start': ent.start_char,\n",
        "                    'end': ent.end_char\n",
        "                })\n",
        "\n",
        "        # Extract key concepts (noun phrases)\n",
        "        concepts = []\n",
        "        for chunk in doc.noun_chunks:\n",
        "            # Adjust length for slightly longer concepts\n",
        "            if 2 <= len(chunk.text.split()) <= 5:\n",
        "                concepts.append({\n",
        "                    'text': chunk.text,\n",
        "                    'pos': chunk.root.pos_,\n",
        "                    'start': chunk.start_char,\n",
        "                    'end': chunk.end_char\n",
        "                })\n",
        "\n",
        "        # Extract sentences\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.split()) >= 10] # Increased minimum sentence length\n",
        "\n",
        "        return {\n",
        "            \"entities\": entities,\n",
        "            \"concepts\": concepts,\n",
        "            \"sentences\": sentences\n",
        "        }\n",
        "\n",
        "    def generate_question_with_t5(self, context: str, answer: str, difficulty: str = \"medium\", question_type: str = \"factual\") -> str:\n",
        "        \"\"\"Generate question using T5 model with prepend approach, considering difficulty.\"\"\"\n",
        "        # Incorporate difficulty into the prompt\n",
        "        prompt_prefix = f\"generate {difficulty} {question_type} question:\"\n",
        "        input_text = f\"{prompt_prefix} context: {context} \\\\n {answer}\"\n",
        "\n",
        "        inputs = self.qg_tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Adjust generation parameters based on difficulty (simple heuristic)\n",
        "        max_length = 100\n",
        "        num_beams = 5\n",
        "        temperature = 0.7\n",
        "        if difficulty == \"easy\":\n",
        "            max_length = 80\n",
        "            temperature = 0.6\n",
        "        elif difficulty == \"hard\":\n",
        "            max_length = 120\n",
        "            temperature = 0.9\n",
        "            num_beams = 8\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.qg_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "\n",
        "        question = self.qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return question.strip()\n",
        "\n",
        "    def generate_question_with_flan(self, context: str, answer: str, difficulty: str = \"medium\", question_type: str = \"factual\") -> str:\n",
        "        \"\"\"Generate question using FLAN-T5 model, considering difficulty.\"\"\"\n",
        "        # Incorporate difficulty into the prompt\n",
        "        prompt = f\"\"\"Given the following context, generate a concise {difficulty}-level short answer question where the answer is '{answer}':\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question:\"\"\"\n",
        "\n",
        "        inputs = self.flan_tokenizer(\n",
        "            prompt,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Adjust generation parameters based on difficulty (simple heuristic)\n",
        "        max_length = 150\n",
        "        num_beams = 4\n",
        "        temperature = 0.8\n",
        "        if difficulty == \"easy\":\n",
        "            max_length = 120\n",
        "            temperature = 0.7\n",
        "        elif difficulty == \"hard\":\n",
        "            max_length = 180\n",
        "            temperature = 0.9\n",
        "            num_beams = 6\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.flan_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        question = self.flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return question.strip()\n",
        "\n",
        "    def classify_question_difficulty(self, question: str, answer: str, context: str) -> str:\n",
        "        \"\"\"Classify question difficulty based on complexity and context.\"\"\"\n",
        "        question_lower = question.lower()\n",
        "        question_words = question_lower.split()\n",
        "        answer_words = answer.lower().split()\n",
        "\n",
        "        # Keyword indicators\n",
        "        easy_keywords = ['what', 'who', 'when', 'where', 'name', 'list', 'define']\n",
        "        medium_keywords = ['how', 'why', 'explain', 'describe', 'role', 'purpose']\n",
        "        hard_keywords = ['analyze', 'evaluate', 'synthesize', 'impact', 'implication', 'relationship']\n",
        "\n",
        "        easy_score = sum(1 for word in easy_keywords if word in question_words)\n",
        "        medium_score = sum(1 for word in medium_keywords if word in question_words)\n",
        "        hard_score = sum(1 for word in hard_keywords if word in question_words)\n",
        "\n",
        "        # Answer length\n",
        "        answer_length_score = 0\n",
        "        if len(answer_words) > 15:\n",
        "            answer_length_score = 3\n",
        "        elif len(answer_words) > 8:\n",
        "            answer_length_score = 2\n",
        "        elif len(answer_words) > 3:\n",
        "            answer_length_score = 1\n",
        "\n",
        "        # Context complexity (simple measure: average sentence length)\n",
        "        sentences = sent_tokenize(context)\n",
        "        avg_sentence_length = np.mean([len(s.split()) for s in sentences]) if sentences else 0\n",
        "\n",
        "        context_complexity_score = 0\n",
        "        if avg_sentence_length > 25:\n",
        "            context_complexity_score = 2\n",
        "        elif avg_sentence_length > 18:\n",
        "            context_complexity_score = 1\n",
        "\n",
        "        # --- Enhanced Linguistic Features ---\n",
        "        if self.nlp:\n",
        "            doc_question = self.nlp(question)\n",
        "            doc_answer = self.nlp(answer)\n",
        "\n",
        "\n",
        "            # 1. Part-of-speech tagging (weights based on complexity)\n",
        "            # Corrected way to get POS counts\n",
        "            pos_counts_question = {}\n",
        "            for token in doc_question:\n",
        "                pos_counts_question[token.pos_] = pos_counts_question.get(token.pos_, 0) + 1\n",
        "\n",
        "            pos_counts_answer = {}\n",
        "            for token in doc_answer:\n",
        "                 pos_counts_answer[token.pos_] = pos_counts_answer.get(token.pos_, 0) + 1\n",
        "\n",
        "\n",
        "            pos_score = (\n",
        "                pos_counts_question.get(spacy.parts_of_speech.ADJ, 0) * 0.6 + # Further Increased weight for Adjectives\n",
        "                pos_counts_question.get(spacy.parts_of_speech.ADV, 0) * 0.7 + # Further Increased weight for Adverbs\n",
        "                pos_counts_question.get(spacy.parts_of_speech.VERB, 0) * 0.5 + # Further Increased weight for Verbs\n",
        "                pos_counts_answer.get(spacy.parts_of_speech.ADJ, 0) * 0.5 +\n",
        "                pos_counts_answer.get(spacy.parts_of_speech.NOUN, 0) * 0.4 # Further Increased weight for Nouns in answer\n",
        "            )\n",
        "\n",
        "\n",
        "            # 2. Dependency parsing complexity (simple measure: average dependency depth) - Higher depth means more complex syntax\n",
        "            dep_depths_question = [len(list(token.ancestors)) for token in doc_question]\n",
        "            avg_dep_depth_question = np.mean(dep_depths_question) if dep_depths_question else 0\n",
        "            dep_score = avg_dep_depth_question * 1.2 # Significantly increased weight\n",
        "\n",
        "\n",
        "            # 3. Named entity recognition - More entities can indicate more specific/complex questions\n",
        "            num_entities_question = len(doc_question.ents)\n",
        "            num_entities_answer = len(doc_answer.ents)\n",
        "            entity_score = (num_entities_question * 1.2 + num_entities_answer * 1.5) # Significantly increased weight for entities\n",
        "\n",
        "\n",
        "            # 4. Lexical diversity (Type-Token Ratio) - Lower TTR might indicate simpler language, higher TTR more complex\n",
        "            question_tokens = [token.text.lower() for token in doc_question if token.is_alpha]\n",
        "            answer_tokens = [token.text.lower() for token in doc_answer if token.is_alpha]\n",
        "\n",
        "            question_ttr = len(set(question_tokens)) / len(question_tokens) if question_tokens else 0\n",
        "            answer_ttr = len(set(answer_tokens)) / len(answer_tokens) if answer_tokens else 0\n",
        "\n",
        "            # Inverse TTR for scoring (lower TTR = higher score for easy, higher TTR = higher score for hard)\n",
        "            # Let's use TTR directly and adjust weights\n",
        "            ttr_score = (question_ttr * 3.0 + answer_ttr * 2.5) # Significantly increased weight\n",
        "\n",
        "\n",
        "            # Combine linguistic features into a single score\n",
        "            linguistic_score = pos_score + dep_score + entity_score + ttr_score\n",
        "\n",
        "        else:\n",
        "            linguistic_score = 0\n",
        "\n",
        "        # Combine all scores with adjusted weights\n",
        "        # Increased weights for hard keywords, answer length, context complexity, and linguistic features\n",
        "        total_score = (\n",
        "            hard_score * 7 + # Further Higher weight for hard keywords\n",
        "            medium_score * 4 + # Increased weight for medium keywords\n",
        "            easy_score * 1.5 + # Slightly increased weight for easy keywords\n",
        "            answer_length_score * 3.0 + # Further Higher weight for answer length\n",
        "            context_complexity_score * 3.0 + # Further Higher weight for context complexity\n",
        "            linguistic_score * 2.5 # Significantly increased weight for combined linguistic features\n",
        "        )\n",
        "\n",
        "        # Refined thresholds based on adjusted scoring\n",
        "        # These thresholds will likely need tuning based on testing\n",
        "        if total_score > 28: # Adjusted thresholds slightly down\n",
        "            return \"hard\"\n",
        "        elif total_score > 14: # Adjusted thresholds slightly down\n",
        "            return \"medium\"\n",
        "        else:\n",
        "            return \"easy\"\n",
        "\n",
        "\n",
        "    def determine_question_type(self, question: str) -> str:\n",
        "        \"\"\"Determine the type of question based on its content.\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        if any(word in question_lower for word in ['what is', 'what are', 'define', 'who is', 'who are', 'when is', 'when did', 'where is', 'where are']):\n",
        "            return \"factual\"\n",
        "        elif any(word in question_lower for word in ['how does', 'how to', 'why is', 'why do', 'explain', 'describe']):\n",
        "            return \"analytical\"\n",
        "        elif any(word in question_lower for word in ['compare', 'contrast', 'differ', 'similarities', 'differences']):\n",
        "            return \"comparative\"\n",
        "        elif any(word in question_lower for word in ['cause', 'effect', 'result', 'lead to', 'consequence']):\n",
        "            return \"causal\"\n",
        "        elif any(word in question_lower for word in ['steps', 'process', 'procedure', 'how to']):\n",
        "            return \"procedural\"\n",
        "        else:\n",
        "            # Use a classifier for better accuracy if available, otherwise default\n",
        "            # try:\n",
        "            #     classification = self.question_classifier(question)[0]\n",
        "            #     # Map classifier labels to our types (requires mapping based on chosen classifier)\n",
        "            #     # This is a placeholder; actual mapping depends on the chosen classifier\n",
        "            #     return 'factual' # Default for now\n",
        "            # except:\n",
        "                 return \"factual\"\n",
        "\n",
        "    def extract_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract keywords from text using NLP.\"\"\"\n",
        "        if not self.nlp:\n",
        "            return []\n",
        "\n",
        "        doc = self.nlp(text)\n",
        "        keywords = []\n",
        "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "        for token in doc:\n",
        "            if (token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'VERB'] and # Include verbs\n",
        "                token.text.lower() not in stopwords and\n",
        "                not token.is_punct and\n",
        "                len(token.text) > 2):\n",
        "                keywords.append(token.text)\n",
        "\n",
        "        # Prioritize multi-word concepts if they exist\n",
        "        multi_word_keywords = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) > 1 and len(chunk.text.split()) <= 3]\n",
        "        keywords = multi_word_keywords + keywords\n",
        "\n",
        "        return list(set(keywords))\n",
        "\n",
        "    def validate_question_answer_pair(self, question: str, expected_answer: str, context: str) -> Dict:\n",
        "        \"\"\"Validate if the question can be answered correctly from the context using the QA model.\"\"\"\n",
        "        try:\n",
        "            # Use QA model and tokenizer explicitly\n",
        "            inputs = self.qa_tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.qa_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            answer_start_scores = outputs.start_logits\n",
        "            answer_end_scores = outputs.end_logits\n",
        "\n",
        "            # Get the most likely answer span\n",
        "            answer_start = torch.argmax(answer_start_scores)\n",
        "            answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "            # Convert tokens to predicted answer string\n",
        "            predicted_answer = self.qa_tokenizer.decode(input_ids[0, answer_start:answer_end], skip_special_tokens=True)\n",
        "\n",
        "            # Calculate a confidence score (using max of start and end logits)\n",
        "            confidence = torch.max(torch.softmax(answer_start_scores, dim=-1)) + torch.max(torch.softmax(answer_end_scores, dim=-1)) / 2.0\n",
        "\n",
        "\n",
        "            # Calculate semantic similarity between expected and predicted answers\n",
        "            # Handle potential errors if encoding fails\n",
        "            try:\n",
        "                expected_embedding = self.sentence_model.encode([expected_answer])\n",
        "                predicted_embedding = self.sentence_model.encode([predicted_answer])\n",
        "                similarity = np.dot(expected_embedding[0], predicted_embedding[0]) / (\n",
        "                    np.linalg.norm(expected_embedding[0]) * np.linalg.norm(predicted_embedding[0])\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error encoding answers for similarity: {e}\")\n",
        "                similarity = 0.0 # Default to 0 similarity on error\n",
        "\n",
        "\n",
        "            # Check if answers are semantically similar or one contains the other\n",
        "            contains_check = (\n",
        "                expected_answer.lower().strip() in predicted_answer.lower().strip() or\n",
        "                predicted_answer.lower().strip() in expected_answer.lower().strip()\n",
        "            )\n",
        "\n",
        "            # Consider similarity and containment for validation\n",
        "            is_valid = (similarity > 0.7 and confidence > 0.4) or (contains_check and confidence > 0.5)\n",
        "\n",
        "            return {\n",
        "                \"is_valid\": is_valid,\n",
        "                \"confidence\": confidence.item(), # Convert tensor to float\n",
        "                \"similarity\": similarity,\n",
        "                \"predicted_answer\": predicted_answer,\n",
        "                \"expected_answer\": expected_answer\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch specific errors from pipeline if possible\n",
        "            print(f\"Error during QA validation: {e}\")\n",
        "            return {\n",
        "                \"is_valid\": False,\n",
        "                \"confidence\": 0.0,\n",
        "                \"similarity\": 0.0,\n",
        "                \"predicted_answer\": \"\",\n",
        "                \"expected_answer\": expected_answer,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def determine_expected_length(self, answer: str) -> str:\n",
        "        \"\"\"Determine expected answer length category based on word count.\"\"\"\n",
        "        word_count = len(answer.split())\n",
        "\n",
        "        if word_count <= 5:\n",
        "            return \"brief (few words)\"\n",
        "        elif word_count <= 15:\n",
        "            return \"short (1-2 sentences)\"\n",
        "        elif word_count <= 30:\n",
        "            return \"medium (2-4 sentences)\"\n",
        "        else:\n",
        "            return \"long (paragraph+)\"\n",
        "\n",
        "\n",
        "    def generate_comprehensive_questions(self, context: str, num_questions: int = 8, difficulty: str = \"medium\") -> List[ShortAnswerQuestion]:\n",
        "        \"\"\"Generate comprehensive set of short answer questions, considering difficulty.\"\"\"\n",
        "        questions = []\n",
        "        generated_pairs = set() # To avoid duplicate question-answer pairs\n",
        "\n",
        "        # Extract key information\n",
        "        key_info = self.extract_key_concepts(context)\n",
        "\n",
        "        # Combine potential answers from entities and concepts\n",
        "        potential_answers = [e['text'] for e in key_info['entities']] + [c['text'] for c in key_info['concepts']]\n",
        "        random.shuffle(potential_answers) # Shuffle to mix entity and concept based questions\n",
        "\n",
        "        # Determine number of attempts per answer based on difficulty\n",
        "        attempts_per_answer_map = {\"easy\": 4, \"medium\": 8, \"hard\": 12} # Increased attempts for all difficulties again\n",
        "        attempts_per_answer = attempts_per_answer_map.get(difficulty, 8)\n",
        "\n",
        "        answers_processed = 0\n",
        "\n",
        "        for answer in potential_answers:\n",
        "            if len(questions) >= num_questions:\n",
        "                break\n",
        "\n",
        "            # Skip if answer is too short or too long\n",
        "            if len(answer.split()) < 2 or len(answer.split()) > 10:\n",
        "                continue\n",
        "\n",
        "            answers_processed += 1\n",
        "            if answers_processed > num_questions * 20: # Further increased limit to try more answers\n",
        "                 print(f\"Reached maximum answer processing attempts ({num_questions * 20}). Stopping.\")\n",
        "                 break\n",
        "            for attempt in range(attempts_per_answer):\n",
        "                if len(questions) >= num_questions:\n",
        "                    break\n",
        "\n",
        "                # Choose which model to use (can alternate or use both)\n",
        "                if attempt % 2 == 0:\n",
        "                    question = self.generate_question_with_t5(context, answer, difficulty=difficulty)\n",
        "                else:\n",
        "                    question = self.generate_question_with_flan(context, answer, difficulty=difficulty)\n",
        "\n",
        "                # Basic cleaning and validation before full QA check\n",
        "                question = question.strip()\n",
        "                if not question or not question.endswith('?') or len(question.split()) < 5:\n",
        "                    continue\n",
        "\n",
        "                # Ensure question is unique\n",
        "                q_a_pair = (question, answer)\n",
        "                if q_a_pair in generated_pairs:\n",
        "                    continue\n",
        "\n",
        "                # Validate question-answer pair\n",
        "                validation = self.validate_question_answer_pair(question, answer, context)\n",
        "\n",
        "                # Filtering based on difficulty and validation confidence\n",
        "                confidence_threshold = 0.3 # Base threshold\n",
        "\n",
        "                # Adjust confidence threshold based on requested difficulty\n",
        "                if difficulty == \"easy\":\n",
        "                    confidence_threshold = 0.25 # Slightly lower threshold for easy questions\n",
        "                elif difficulty == \"hard\":\n",
        "                     confidence_threshold = 0.35 # Slightly higher threshold for hard questions\n",
        "\n",
        "\n",
        "                if validation[\"is_valid\"] and validation[\"confidence\"] > confidence_threshold: # Apply confidence threshold\n",
        "                    # Determine question type and classified difficulty\n",
        "                    question_type = self.determine_question_type(question)\n",
        "                    classified_difficulty = self.classify_question_difficulty(question, answer, context) # Classify generated question's actual difficulty\n",
        "\n",
        "                    # Add the question if its classified difficulty is the requested one or one level below\n",
        "                    # This allows some flexibility while aiming for the target difficulty\n",
        "                    difficulty_levels = [\"easy\", \"medium\", \"hard\"]\n",
        "                    requested_index = difficulty_levels.index(difficulty)\n",
        "                    classified_index = difficulty_levels.index(classified_difficulty)\n",
        "\n",
        "                    # Accept if classified difficulty is at or one level below requested difficulty\n",
        "                    if classified_index >= requested_index or (requested_index > 0 and classified_index == requested_index - 1):\n",
        "\n",
        "                        keywords = self.extract_keywords(f\"{question} {answer}\")\n",
        "                        expected_length = self.determine_expected_length(answer)\n",
        "\n",
        "                        saq = ShortAnswerQuestion(\n",
        "                            question=question,\n",
        "                            answer=answer,\n",
        "                            context_sentence=context[:200] + \"...\" if len(context) > 200 else context,\n",
        "                            question_type=question_type,\n",
        "                            difficulty=classified_difficulty, # Use classified difficulty\n",
        "                            confidence=validation[\"confidence\"],\n",
        "                            keywords=keywords[:5],\n",
        "                            expected_length=expected_length\n",
        "                        )\n",
        "                        questions.append(saq)\n",
        "                        generated_pairs.add(q_a_pair) # Add to history\n",
        "                        # If we found a question of the requested difficulty, move to the next answer\n",
        "                        if classified_difficulty == difficulty:\n",
        "                            break\n",
        "\n",
        "\n",
        "        # Fallback: Generate questions directly from sentences if not enough generated\n",
        "        if len(questions) < num_questions:\n",
        "             print(f\"Warning: Could not generate {num_questions} questions of the requested difficulty. Adding fallback questions.\")\n",
        "             for sentence in key_info[\"sentences\"]:\n",
        "                 if len(questions) >= num_questions:\n",
        "                     break\n",
        "\n",
        "                 # Generate a question based on the sentence (can use template or model)\n",
        "                 # Simple template fallback\n",
        "                 question = f\"What is discussed in the sentence: \\\"{sentence[:50]}...\\\"?\"\n",
        "                 answer = sentence # The sentence itself is the \"answer\" in this case\n",
        "\n",
        "                 # Validate (less strict for fallback)\n",
        "                 validation = self.validate_question_answer_pair(question, answer, context)\n",
        "\n",
        "                 # Even if not perfectly valid, added as a fallback if needed and unique\n",
        "                 q_a_pair = (question, answer)\n",
        "                 if q_a_pair not in generated_pairs:\n",
        "                     difficulty = \"easy\" # Fallback questions are usually easy\n",
        "                     question_type = \"factual\"\n",
        "                     keywords = self.extract_keywords(sentence)[:5]\n",
        "                     expected_length = self.determine_expected_length(answer)\n",
        "\n",
        "                     saq = ShortAnswerQuestion(\n",
        "                         question=question,\n",
        "                         answer=\"Key points from the sentence.\", # Placeholder answer\n",
        "                         context_sentence=sentence,\n",
        "                         question_type=question_type,\n",
        "                         difficulty=difficulty,\n",
        "                         confidence=validation[\"confidence\"] if validation[\"is_valid\"] else 0.1, # Low confidence for fallback\n",
        "                         keywords=keywords,\n",
        "                         expected_length=\"short (1-2 sentences)\"\n",
        "                     )\n",
        "                     questions.append(saq)\n",
        "                     generated_pairs.add(q_a_pair)\n",
        "\n",
        "\n",
        "        # Sort by confidence (or potentially by classified difficulty later) and return\n",
        "        questions.sort(key=lambda x: x.confidence, reverse=True)\n",
        "        return questions[:num_questions]\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the advanced SAQ generator.\"\"\"\n",
        "    generator = AdvancedShortAnswerGenerator()\n",
        "\n",
        "    # Sample context about machine learning\n",
        "    sample_context = \"\"\"\n",
        "    Machine learning is a subset of artificial intelligence that enables computers to learn and improve\n",
        "    from experience without being explicitly programmed. It involves algorithms that can identify patterns\n",
        "    in data and make predictions or decisions based on those patterns. There are three main types of machine\n",
        "    learning: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning\n",
        "    uses labeled training data to learn a mapping from inputs to outputs. Popular supervised learning\n",
        "    algorithms include linear regression, decision trees, and neural networks. Unsupervised learning finds\n",
        "    hidden patterns in data without labeled examples, using techniques like clustering and dimensionality\n",
        "    reduction. Reinforcement learning involves an agent learning to make decisions by receiving rewards\n",
        "    or penalties for actions taken in an environment. Deep learning, a subset of machine learning, uses\n",
        "    artificial neural networks with multiple layers to model complex patterns in data. Applications of\n",
        "    machine learning include image recognition, natural language processing, recommendation systems,\n",
        "    and autonomous vehicles. The field has seen rapid growth due to increased computational power,\n",
        "    large datasets, and improved algorithms.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Advanced Short Answer Question Generator\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get user input\n",
        "    user_context = input(\"Enter your context (or press Enter to use sample): \").strip()\n",
        "    if not user_context:\n",
        "        user_context = sample_context\n",
        "        print(\"Using sample context about machine learning...\")\n",
        "\n",
        "    try:\n",
        "        num_questions = int(input(\"Number of questions to generate (default 6): \") or \"6\")\n",
        "    except ValueError:\n",
        "        num_questions = 6\n",
        "    print(f\"\\nGenerating {num_questions} short answer questions...\")\n",
        "\n",
        "    # Generate questions, passing the difficulty\n",
        "    questions = generator.generate_comprehensive_questions(user_context, num_questions)\n",
        "\n",
        "    # Display results\n",
        "    if questions:\n",
        "        for i, q in enumerate(questions, 1):\n",
        "            print(f\"\\nQuestion {i}: [CLASSIFIED: {q.difficulty.upper()}] ({q.question_type})\") # Display classified difficulty\n",
        "            print(f\"Q: {q.question}\")\n",
        "            print(f\"A: {q.answer}\")\n",
        "            print(f\"Expected Length: {q.expected_length}\")\n",
        "    else:\n",
        "        print(\"No high-quality questions could be generated from the provided context.\")\n",
        "        print(\"Try providing a longer, more detailed context with specific information.\")\n",
        "\n",
        "    print(\"\\nGeneration complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkkPD9QA-2Z8",
        "outputId": "1a1775d9-d426-44d0-dc2a-64d0f167ff6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sentence transformer...\n",
            "Advanced Short Answer Question Generator\n",
            "============================================================\n",
            "Enter your context (or press Enter to use sample): \n",
            "Using sample context about machine learning...\n",
            "Number of questions to generate (default 6): 4\n",
            "\n",
            "Generating 4 short answer questions...\n",
            "\n",
            "Question 1: [CLASSIFIED: MEDIUM] (factual)\n",
            "Q: What type of data is deep learning used to model?\n",
            "A: complex patterns\n",
            "Expected Length: brief (few words)\n",
            "\n",
            "Question 2: [CLASSIFIED: MEDIUM] (factual)\n",
            "Q: what is the first type of machine learning?\n",
            "A: reinforcement learning\n",
            "Expected Length: brief (few words)\n",
            "\n",
            "Question 3: [CLASSIFIED: MEDIUM] (factual)\n",
            "Q: Machine learning is a subset of AI that allows computers to learn and improve from experience without being explicitly programmed?\n",
            "A: decision trees\n",
            "Expected Length: brief (few words)\n",
            "\n",
            "Question 4: [CLASSIFIED: MEDIUM] (factual)\n",
            "Q: Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed?\n",
            "A: a subset\n",
            "Expected Length: brief (few words)\n",
            "\n",
            "Generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFvGMWctDueq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}