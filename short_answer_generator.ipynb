{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9geFhx2LxpKcLjq1rwpK/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DishaKushwah/custom-quiz-generator/blob/main/short_answer_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8kqiMR4-nfk",
        "outputId": "aef8799d-e2d0-4868-a31a-23f9f58d7bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.1\n",
            "    Uninstalling transformers-4.53.1:\n",
            "      Successfully uninstalled transformers-4.53.1\n",
            "Successfully installed transformers-4.53.2\n"
          ]
        }
      ],
      "source": [
        "## SHORT ANSWERS\n",
        "%pip install --upgrade transformers\n",
        "import torch\n",
        "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering, pipeline, T5ForConditionalGeneration, T5Tokenizer)\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ShortAnswerQuestion:\n",
        "    question: str\n",
        "    answer: str\n",
        "    context_sentence: str\n",
        "    question_type: str\n",
        "    difficulty: str\n",
        "    confidence: float\n",
        "    keywords: List[str]\n",
        "    expected_length: str\n",
        "\n",
        "class AdvancedShortAnswerGenerator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize with state-of-the-art models for question generation.\"\"\"\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Load the best question generation model - T5-large fine-tuned for QG\n",
        "        self.qg_model_name = \"valhalla/t5-base-qg-hl\"\n",
        "\n",
        "        # Use AutoTokenizer and AutoModelForSeq2SeqLM for broader compatibility\n",
        "        self.qg_tokenizer = AutoTokenizer.from_pretrained(self.qg_model_name)\n",
        "        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(self.qg_model_name).to(self.device)\n",
        "\n",
        "        # Load FLAN-T5 for better question generation\n",
        "        self.flan_model_name = \"google/flan-t5-base\"\n",
        "        self.flan_tokenizer = AutoTokenizer.from_pretrained(self.flan_model_name)\n",
        "        self.flan_model = AutoModelForSeq2SeqLM.from_pretrained(self.flan_model_name).to(self.device)\n",
        "\n",
        "        # Load DeBERta for high-quality answer extraction\n",
        "        self.qa_model_name = \"deepset/roberta-base-squad2\"\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(self.qa_model_name)\n",
        "        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(self.qa_model_name).to(self.device)\n",
        "\n",
        "        # Load sentence transformer for semantic analysis\n",
        "        self.sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "        # Load spaCy for advanced NLP\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = None\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            nltk.download('stopwords', quiet=True)\n",
        "            nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "            nltk.download('punkt_tab', quiet=True) # Added download for punkt_tab\n",
        "        except:\n",
        "            pass\n",
        "        # Question type templates\n",
        "        self.question_templates = {\n",
        "            'factual': [\"What is {}?\",\"What does {} mean?\",\"What are the characteristics of {}?\",\"Define {}.\",\"Explain {}.\"],\n",
        "            'analytical': [\"How does {} work?\",\"Why is {} important?\",\"What is the significance of {}?\",\"How does {} relate to {}?\",\"What are the implications of {}?\"],\n",
        "            'comparative': [\"Compare {} and {}.\",\"What are the differences between {} and {}?\",\"How does {} differ from {}?\",\"What are the similarities between {} and {}?\"],\n",
        "            'causal': [\"What caused {}?\",\"What are the effects of {}?\",\"How did {} lead to {}?\",\"What resulted from {}?\"],\n",
        "            'procedural': [\"How do you {}?\",\"What are the steps to {}?\",\"Describe the process of {}.\",\"What is the procedure for {}?\"]}\n",
        "\n",
        "    def extract_key_concepts(self, text: str) -> Dict:\n",
        "        \"\"\"Extract key concepts and entities from text.\"\"\"\n",
        "        if not self.nlp:\n",
        "            return {\"entities\": [], \"concepts\": [], \"sentences\": []}\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            # Include more entity types for broader question generation\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'CARDINAL', 'ORDINAL', 'NORP', 'LOC', 'PRODUCT']:\n",
        "                entities.append({'text': ent.text,'label': ent.label_,'start': ent.start_char,'end': ent.end_char})\n",
        "\n",
        "        # Extract key concepts (noun phrases)\n",
        "        concepts = []\n",
        "        for chunk in doc.noun_chunks:\n",
        "            # Adjust length for slightly longer concepts\n",
        "            if 2 <= len(chunk.text.split()) <= 5:\n",
        "                concepts.append({'text': chunk.text,'pos': chunk.root.pos_,'start': chunk.start_char,'end': chunk.end_char})\n",
        "\n",
        "        # Extract sentences\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.split()) >= 10] # Increased minimum sentence length\n",
        "        return {\"entities\": entities,\"concepts\": concepts,\"sentences\": sentences}\n",
        "\n",
        "    def generate_question_with_t5(self, context: str, answer: str, difficulty: str = \"medium\", question_type: str = \"factual\") -> str:\n",
        "        \"\"\"Generate question using T5 model with prepend approach, considering difficulty.\"\"\"\n",
        "        # Incorporate difficulty into the prompt\n",
        "        prompt_prefix = f\"generate {difficulty} {question_type} question:\"\n",
        "        input_text = f\"{prompt_prefix} context: {context} \\\\n {answer}\"\n",
        "        inputs = self.qg_tokenizer.encode_plus(input_text,max_length=512,truncation=True,padding=True,return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Adjust generation parameters based on difficulty (simple heuristic)\n",
        "        max_length = 100\n",
        "        num_beams = 5\n",
        "        temperature = 0.7\n",
        "        if difficulty == \"easy\":\n",
        "            max_length = 80\n",
        "            temperature = 0.6\n",
        "        elif difficulty == \"hard\":\n",
        "            max_length = 120\n",
        "            temperature = 0.9\n",
        "            num_beams = 8\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.qg_model.generate(inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"],max_length=max_length,num_beams=num_beams,temperature=temperature,do_sample=True,early_stopping=True,no_repeat_ngram_size=2)\n",
        "        question = self.qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return question.strip()\n",
        "\n",
        "    def generate_question_with_flan(self, context: str, answer: str, difficulty: str = \"medium\", question_type: str = \"factual\") -> str:\n",
        "        \"\"\"Generate question using FLAN-T5 model, considering difficulty.\"\"\"\n",
        "        # Incorporate difficulty into the prompt\n",
        "        prompt = f\"\"\"Given the following context, generate a concise {difficulty}-level short answer question where the answer is '{answer}':\n",
        "Context: {context}\n",
        "Question:\"\"\"\n",
        "        inputs = self.flan_tokenizer(prompt,max_length=512,truncation=True,padding=True,return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Adjust generation parameters based on difficulty (simple heuristic)\n",
        "        max_length = 150\n",
        "        num_beams = 4\n",
        "        temperature = 0.8\n",
        "        if difficulty == \"easy\":\n",
        "            max_length = 100\n",
        "            temperature = 0.6\n",
        "        elif difficulty == \"hard\":\n",
        "            max_length = 200\n",
        "            temperature = 0.9\n",
        "            num_beams = 6\n",
        "        with torch.no_grad():\n",
        "            outputs = self.flan_model.generate(inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"],max_length=max_length,num_beams=num_beams,temperature=temperature,do_sample=True,early_stopping=True)\n",
        "\n",
        "        question = self.flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return question.strip()\n",
        "\n",
        "    def classify_question_difficulty(self, question: str, answer: str, context: str) -> str:\n",
        "        \"\"\"Classify question difficulty based on complexity and context.\"\"\"\n",
        "        if not self.nlp:\n",
        "            return \"medium\" # Default to medium if spaCy is not loaded\n",
        "\n",
        "        question_lower = question.lower()\n",
        "        question_words = question_lower.split()\n",
        "        answer_words = answer.lower().split()\n",
        "\n",
        "        # Keyword indicators\n",
        "        easy_keywords = ['what', 'who', 'when', 'where', 'name', 'list', 'define']\n",
        "        medium_keywords = ['how', 'why', 'explain', 'describe', 'role', 'purpose']\n",
        "        hard_keywords = ['analyze', 'evaluate', 'synthesize', 'impact', 'implication', 'relationship']\n",
        "        easy_score = sum(1 for word in easy_keywords if word in question_words)\n",
        "        medium_score = sum(1 for word in medium_keywords if word in question_words)\n",
        "        hard_score = sum(1 for word in hard_keywords if word in question_words)\n",
        "\n",
        "        # Answer length\n",
        "        answer_length_score = 0\n",
        "        if len(answer_words) > 15:\n",
        "            answer_length_score = 3\n",
        "        elif len(answer_words) > 8:\n",
        "            answer_length_score = 2\n",
        "        elif len(answer_words) > 3:\n",
        "            answer_length_score = 1\n",
        "\n",
        "        # Context complexity (simple measure: average sentence length)\n",
        "        sentences = sent_tokenize(context)\n",
        "        avg_sentence_length = np.mean([len(s.split()) for s in sentences]) if sentences else 0\n",
        "\n",
        "        context_complexity_score = 0\n",
        "        if avg_sentence_length > 25:\n",
        "            context_complexity_score = 2\n",
        "        elif avg_sentence_length > 18:\n",
        "            context_complexity_score = 1\n",
        "        if self.nlp:\n",
        "            doc_question = self.nlp(question)\n",
        "            doc_answer = self.nlp(answer)\n",
        "            # 1. Part-of-speech tagging (weights based on complexity)\n",
        "            # Corrected way to get POS counts\n",
        "            pos_counts_question = {}\n",
        "            for token in doc_question:\n",
        "                pos_counts_question[token.pos_] = pos_counts_question.get(token.pos_, 0) +1\n",
        "            pos_counts_answer = {}\n",
        "            for token in doc_answer:\n",
        "                 pos_counts_answer[token.pos_] = pos_counts_answer.get(token.pos_, 0) +1\n",
        "            pos_score = (\n",
        "                pos_counts_question.get(spacy.parts_of_speech.ADJ, 0) * 0.6 + # Further Increased weight for Adjectives\n",
        "                pos_counts_question.get(spacy.parts_of_speech.ADV, 0) * 0.7 + # Further Increased weight for Adverbs\n",
        "                pos_counts_question.get(spacy.parts_of_speech.VERB, 0) * 0.5 + # Further Increased weight for Verbs\n",
        "                pos_counts_answer.get(spacy.parts_of_speech.ADJ, 0) * 0.5 +\n",
        "                pos_counts_answer.get(spacy.parts_of_speech.NOUN, 0) * 0.4 )# Further Increased weight for Nouns in answer\n",
        "\n",
        "            # 2. Dependency parsing complexity (simple measure: average dependency depth) - Higher depth means more complex syntax\n",
        "            dep_depths_question = [len(list(token.ancestors)) for token in doc_question]\n",
        "            avg_dep_depth_question = np.mean(dep_depths_question) if dep_depths_question else 0\n",
        "            dep_score = avg_dep_depth_question * 1.2 # Significantly increased weigh\n",
        "\n",
        "                                  # 3. Named entity recognition - More entities can indicate more specific/complex questions\n",
        "            num_entities_question = len(doc_question.ents)\n",
        "            num_entities_answer = len(doc_answer.ents)\n",
        "            entity_score = (num_entities_question * 1.2 + num_entities_answer * 1.5) # Significantly increased weight for entitie\n",
        "\n",
        "                                  # 4. Lexical diversity (Type-Token Ratio) - Lower TTR might indicate simpler language, higher TTR more complex\n",
        "            question_tokens = [token.text.lower() for token in doc_question if token.is_alpha]\n",
        "            answer_tokens = [token.text.lower() for token in doc_answer if token.is_alpha]\n",
        "            question_ttr = len(set(question_tokens)) / len(question_tokens) if question_tokens else 0\n",
        "            answer_ttr = len(set(answer_tokens)) / len(answer_tokens) if answer_tokens else 0\n",
        "\n",
        "            # Inverse TTR for scoring (lower TTR = higher score for easy, higher TTR = higher score for hard)\n",
        "            ttr_score = (question_ttr * 3.0 + answer_ttr * 2.5) # Significantly increased weigh\n",
        "            # Combine linguistic features into a single score\n",
        "            linguistic_score = pos_score + dep_score + entity_score + ttr_score\n",
        "        else:\n",
        "            linguistic_score = 0\n",
        "\n",
        "        # Combine all scores with adjusted weights\n",
        "        total_score = (hard_score * 7 + medium_score * 4 + easy_score * 1.5 + answer_length_score * 3.0 + context_complexity_score * 3.0 + linguistic_score * 2.5 )\n",
        "\n",
        "        # Refined thresholds based on adjusted scoring\n",
        "        # These thresholds will likely need tuning based on testing\n",
        "        if total_score > 28: # Adjusted thresholds slightly down\n",
        "            return \"hard\"\n",
        "        elif total_score > 14: # Adjusted thresholds slightly down\n",
        "            return \"medium\"\n",
        "        else:\n",
        "            return \"easy\"\n",
        "\n",
        "    def determine_question_type(self, question: str) -> str:\n",
        "        \"\"\"Determine the type of question based on its content.\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        if any(word in question_lower for word in ['what is', 'what are', 'define', 'who is', 'who are', 'when is', 'when did', 'where is', 'where are']):\n",
        "            return \"factual\"\n",
        "        elif any(word in question_lower for word in ['how does', 'how to', 'why is', 'why do', 'explain', 'describe']):\n",
        "            return \"analytical\"\n",
        "        elif any(word in question_lower for word in ['compare', 'contrast', 'differ', 'similarities', 'differences']):\n",
        "            return \"comparative\"\n",
        "        elif any(word in question_lower for word in ['cause', 'effect', 'result', 'lead to', 'consequence']):\n",
        "            return \"causal\"\n",
        "        elif any(word in question_lower for word in ['steps', 'process', 'procedure', 'how to']):\n",
        "            return \"procedural\"\n",
        "        else:\n",
        "            return \"factual\"\n",
        "\n",
        "    def extract_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract keywords from text using NLP.\"\"\"\n",
        "        if not self.nlp:\n",
        "            return []\n",
        "\n",
        "        doc = self.nlp(text)\n",
        "        keywords = []\n",
        "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "        for token in doc:\n",
        "            if (token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'VERB'] and token.text.lower() not in stopwords and not token.is_punct and len(token.text) > 2):\n",
        "                keywords.append(token.text)\n",
        "\n",
        "        # Prioritize multi-word concepts if they exist\n",
        "        multi_word_keywords = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) > 1 and len(chunk.text.split()) <= 3]\n",
        "        keywords = multi_word_keywords + keywords\n",
        "        return list(set(keywords))\n",
        "\n",
        "    def validate_question_answer_pair(self, question: str, expected_answer: str, context: str) -> Dict:\n",
        "        \"\"\"Validate if the question can be answered correctly from the context using the QA model.\"\"\"\n",
        "        try:\n",
        "            # Use QA model and tokenizer explicitly\n",
        "            inputs = self.qa_tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.qa_model(input_ids=input_ids, attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "            answer_start_scores = outputs.start_logits\n",
        "            answer_end_scores = outputs.end_logits\n",
        "\n",
        "            # Get the most likely answer span\n",
        "            answer_start = torch.argmax(answer_start_scores)\n",
        "            answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "            # Convert tokens to predicted answer string\n",
        "            predicted_answer = self.qa_tokenizer.decode(input_ids[0, answer_start:answer_end], skip_special_tokens=True)\n",
        "\n",
        "            # Calculate a confidence score (using max of start and end logits)\n",
        "            confidence = (torch.max(torch.softmax(answer_start_scores, dim=-1)) + torch.max(torch.softmax(answer_end_scores, dim=-1))) / 2.0\n",
        "\n",
        "            # Calculate semantic similarity between expected and predicted answers\n",
        "            # Handle potential errors if encoding fails\n",
        "            try:\n",
        "                expected_embedding = self.sentence_model.encode([expected_answer])\n",
        "                predicted_embedding = self.sentence_model.encode([predicted_answer])\n",
        "                similarity = np.dot(expected_embedding[0], predicted_embedding[0]) / (np.linalg.norm(expected_embedding[0]) * np.linalg.norm(predicted_embedding[0]))\n",
        "            except Exception as e:\n",
        "                print(f\"Error encoding answers for similarity: {e}\")\n",
        "                similarity = 0.0 # Default to 0 similarity on error\n",
        "\n",
        "            # Check if answers are semantically similar or one contains the other\n",
        "            contains_check = (expected_answer.lower().strip() in predicted_answer.lower().strip() or predicted_answer.lower().strip() in expected_answer.lower().strip())\n",
        "\n",
        "            # Consider similarity and containment for validation\n",
        "            is_valid = (similarity > 0.7 and confidence > 0.4) or (contains_check and confidence > 0.5)\n",
        "            return {\"is_valid\": is_valid,\"confidence\": confidence.item(),\"similarity\": similarity,\"predicted_answer\": predicted_answer,\"expected_answer\": expected_answer}\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch specific errors from pipeline if possible\n",
        "            print(f\"Error during QA validation: {e}\")\n",
        "            return {\"is_valid\": False,\"confidence\": 0.0,\"similarity\": 0.0,\"predicted_answer\": \"\",\"expected_answer\": expected_answer,\"error\": str(e)}\n",
        "\n",
        "    def determine_expected_length(self, answer: str) -> str:\n",
        "        \"\"\"Determine expected answer length category based on word count.\"\"\"\n",
        "        word_count = len(answer.split())\n",
        "\n",
        "        if word_count <= 5:\n",
        "            return \"brief (few words)\"\n",
        "        elif word_count <= 15:\n",
        "            return \"short (1-2 sentences)\"\n",
        "        elif word_count <= 30:\n",
        "            return \"medium (2-4 sentences)\"\n",
        "        else:\n",
        "            return \"long (paragraph+)\"\n",
        "\n",
        "    def generate_comprehensive_questions(self, context: str, num_questions: int = 8, difficulty: str = \"medium\") -> List[ShortAnswerQuestion]:\n",
        "        \"\"\"Generate comprehensive set of short answer questions, considering difficulty.\"\"\"\n",
        "        questions = []\n",
        "        generated_pairs = set() # To avoid duplicate question-answer pairs\n",
        "\n",
        "        # Extract key information\n",
        "        key_info = self.extract_key_concepts(context)\n",
        "\n",
        "        # Combine potential answers from entities and concepts\n",
        "        potential_answers = [e['text'] for e in key_info['entities']] + [c['text'] for c in key_info['concepts']]\n",
        "        random.shuffle(potential_answers) # Shuffle to mix entity and concept based questions\n",
        "\n",
        "        # Determine number of attempts per answer based on difficulty\n",
        "        attempts_per_answer_map = {\"easy\": 4, \"medium\": 8, \"hard\": 12} # Increased attempts for all difficulties again\n",
        "        attempts_per_answer = attempts_per_answer_map.get(difficulty, 8)\n",
        "\n",
        "        answers_processed = 0\n",
        "        for answer in potential_answers:\n",
        "            if len(questions) >= num_questions:\n",
        "                break\n",
        "            answers_processed += 1\n",
        "            if answers_processed > num_questions * 20: # Further increased limit to try more answers\n",
        "                 print(f\"Reached maximum answer processing attempts ({num_questions * 20}). Stopping.\")\n",
        "                 break\n",
        "            for attempt in range(attempts_per_answer):\n",
        "                if len(questions) >= num_questions:\n",
        "                    break\n",
        "\n",
        "                # Choose which model to use (can alternate or use both)\n",
        "                if attempt % 2 == 0:\n",
        "                    question = self.generate_question_with_t5(context, answer, difficulty=difficulty)\n",
        "                else:\n",
        "                    question = self.generate_question_with_flan(context, answer, difficulty=difficulty)\n",
        "\n",
        "                # Basic cleaning and validation before full QA check\n",
        "                question = question.strip()\n",
        "                if not question or not question.endswith('?') or len(question.split()) < 5:\n",
        "                    continue\n",
        "\n",
        "                # Ensure question is unique\n",
        "                q_a_pair = (question, answer)\n",
        "                if q_a_pair in generated_pairs:\n",
        "                    continue\n",
        "\n",
        "                # Validate question-answer pair\n",
        "                validation = self.validate_question_answer_pair(question, answer, context)\n",
        "\n",
        "                # Filtering based on difficulty and validation confidence\n",
        "                confidence_threshold = 0.3 # Base threshold\n",
        "\n",
        "                # Adjust confidence threshold based on requested difficulty\n",
        "                if difficulty == \"easy\":\n",
        "                    confidence_threshold = 0.25 #lower threshold for easy questions\n",
        "                elif difficulty == \"hard\":\n",
        "                     confidence_threshold = 0.35 #higher threshold for hard questions\n",
        "\n",
        "\n",
        "                if validation[\"is_valid\"] and validation[\"confidence\"] > confidence_threshold: # confidence threshold\n",
        "                    #type and classified difficulty\n",
        "                    question_type = self.determine_question_type(question)\n",
        "                    # Check if nlp is loaded before classifying difficulty\n",
        "                    if self.nlp:\n",
        "                        classified_difficulty = self.classify_question_difficulty(question, answer, context) # Classify generated question's actual difficulty\n",
        "                    else:\n",
        "                        classified_difficulty = \"medium\" # Default to medium if spaCy not loaded\n",
        "\n",
        "                    # Add the question if its classified difficulty is the requested one or one level below\n",
        "                    # This allows some flexibility while aiming for the target difficulty\n",
        "                    difficulty_levels = [\"easy\", \"medium\", \"hard\"]\n",
        "                    requested_index = difficulty_levels.index(difficulty)\n",
        "                    classified_index = difficulty_levels.index(classified_difficulty)\n",
        "\n",
        "                    # Accept if classified difficulty is at or one level below requested difficulty\n",
        "                    if classified_index >= requested_index or (requested_index > 0 and classified_index == requested_index - 1):\n",
        "\n",
        "                        keywords = self.extract_keywords(f\"{question} {answer}\")\n",
        "                        expected_length = self.determine_expected_length(answer)\n",
        "\n",
        "                        saq = ShortAnswerQuestion(question=question,answer=answer,context_sentence=context[:200] + \"...\" if len(context) > 200 else context,question_type=question_type,difficulty=classified_difficulty,confidence=validation[\"confidence\"],keywords=keywords[:5],expected_length=expected_length)\n",
        "                        questions.append(saq)\n",
        "                        generated_pairs.add(q_a_pair) # Add to history\n",
        "                        # If we found a question of the requested difficulty, move to the next answer\n",
        "                        if classified_difficulty == difficulty:\n",
        "                            break\n",
        "        # Fallback: Generate questions directly from sentences if not enough generated\n",
        "        if len(questions) < num_questions:\n",
        "             print(f\"Warning: Could not generate {num_questions} questions of the requested difficulty. Adding fallback questions.\")\n",
        "             for sentence in key_info[\"sentences\"]:\n",
        "                 if len(questions) >= num_questions:\n",
        "                     break\n",
        "\n",
        "                 # Generate a question based on the sentence (can use template or model)\n",
        "                 # Simple template fallback\n",
        "                 question = f\"What is discussed in the sentence: \\\"{sentence[:50]}...\\\"?\"\n",
        "                 answer = sentence # The sentence itself is the \"answer\" in this case\n",
        "\n",
        "                 # Validate (less strict for fallback)\n",
        "                 validation = self.validate_question_answer_pair(question, answer, context)\n",
        "\n",
        "                 # Even if not perfectly valid, added as a fallback if needed and unique\n",
        "                 q_a_pair = (question, answer)\n",
        "                 if q_a_pair not in generated_pairs:\n",
        "                     difficulty = \"easy\" # Fallback questions are usually easy\n",
        "                     question_type = \"factual\"\n",
        "                     keywords = self.extract_keywords(sentence)[:5]\n",
        "                     expected_length = self.determine_expected_length(answer)\n",
        "\n",
        "                     saq = ShortAnswerQuestion( question=question, answer=\"Key points from the sentence.\", context_sentence=sentence, question_type=question_type, difficulty=difficulty, confidence=validation[\"confidence\"] if validation[\"is_valid\"] else 0.1, keywords=keywords, expected_length=\"short (1-2 sentences)\")\n",
        "                     questions.append(saq)\n",
        "                     generated_pairs.add(q_a_pair)\n",
        "\n",
        "        # Sort by confidence (or potentially by classified difficulty later) and return\n",
        "        questions.sort(key=lambda x: x.confidence, reverse=True)\n",
        "        return questions[:num_questions]\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the advanced SAQ generator.\"\"\"\n",
        "    generator = AdvancedShortAnswerGenerator()\n",
        "\n",
        "    print(\" Short Answer Question Generator\")\n",
        "    # Get user input\n",
        "    user_context = input(\"Enter your context (or press Enter to use sample): \").strip()\n",
        "\n",
        "    try:\n",
        "        num_questions = int(input(\"Number of questions to generate (default 6): \") or \"6\")\n",
        "    except ValueError:\n",
        "        num_questions = 6\n",
        "    print(f\"\\nGenerating {num_questions} short answer questions...\")\n",
        "\n",
        "    # Generate questions, passing the difficulty\n",
        "    questions = generator.generate_comprehensive_questions(user_context, num_questions)\n",
        "\n",
        "    # Display results\n",
        "    if questions:\n",
        "        for i, q in enumerate(questions, 1):\n",
        "            print(f\"\\nQuestion {i}: [CLASSIFIED: {q.difficulty.upper()}] ({q.question_type})\") # Display classified difficulty\n",
        "            print(f\"Q: {q.question}\")\n",
        "            print(f\"A: {q.answer}\")\n",
        "            print(f\"Expected Length: {q.expected_length}\")\n",
        "    else:\n",
        "        print(\"No high-quality questions could be generated from the provided context.\")\n",
        "        print(\"Try providing a longer, more detailed context with specific information.\")\n",
        "\n",
        "    print(\"\\nGeneration complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkkPD9QA-2Z8",
        "outputId": "4533385c-10e3-474e-bba3-51d40466b07c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Short Answer Question Generator\n",
            "Enter your context (or press Enter to use sample): India has numerous national parks dedicated to preserving wildlife and biodiversity. Some of the most famous include Jim Corbett National Park in Uttarakhand, known for tigers; Kaziranga National Park in Assam, home to the one-horned rhinoceros; and Sundarbans in West Bengal, famous for mangrove forests and Royal Bengal Tigers. These parks also support eco-tourism and help protect endangered species and fragile ecosystems.\n",
            "Number of questions to generate (default 6): 3\n",
            "\n",
            "Generating 3 short answer questions...\n",
            "\n",
            "Question 1: [CLASSIFIED: HARD] (factual)\n",
            "Q: What national park in Assam is home to the one-horned rhinoceros?\n",
            "A: Kaziranga National Park\n",
            "Expected Length: brief (few words)\n",
            "\n",
            "Question 2: [CLASSIFIED: HARD] (factual)\n",
            "Q: What national park is home to the one-horned rhinoceros?\n",
            "A: Kaziranga National Park\n",
            "Expected Length: brief (few words)\n",
            "\n",
            "Question 3: [CLASSIFIED: MEDIUM] (factual)\n",
            "Q: Which type of ecosystems are protected by national parks?\n",
            "A: fragile ecosystems\n",
            "Expected Length: brief (few words)\n",
            "\n",
            "Generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFvGMWctDueq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}