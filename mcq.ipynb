{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/hGIOI6EtABYhLUrZsHym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DishaKushwah/custom-quiz-generator/blob/main/mcq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZwckNcP90hL"
      },
      "outputs": [],
      "source": [
        "## MCQS\n",
        "import torch\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5Tokenizer,\n",
        "    pipeline, AutoTokenizer, AutoModel\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "class MultipleChoiceQuestionGenerator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the MCQ generator with advanced models.\"\"\"\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load T5 model for question generation\n",
        "        self.qg_model_name = \"valhalla/t5-base-qg-hl\"\n",
        "        self.qg_tokenizer = T5Tokenizer.from_pretrained(self.qg_model_name)\n",
        "        self.qg_model = T5ForConditionalGeneration.from_pretrained(self.qg_model_name).to(self.device)\n",
        "\n",
        "        # Load question-answering pipeline for answer validation\n",
        "        self.qa_pipeline = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=\"deepset/roberta-large-squad2\",\n",
        "            tokenizer=\"deepset/roberta-large-squad2\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "        # Load sentence transformer for semantic similarity (distractor generation)\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Load spaCy for NLP processing\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = None\n",
        "\n",
        "        # Load fill-mask pipeline for generating distractors\n",
        "        self.fill_mask = pipeline(\n",
        "            \"fill-mask\",\n",
        "            model=\"roberta-large\",\n",
        "            tokenizer=\"roberta-large\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "        # Download NLTK data\n",
        "        try:\n",
        "            nltk.download('wordnet', quiet=True)\n",
        "            nltk.download('omw-1.4', quiet=True)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def extract_key_information(self, text: str) -> Dict:\n",
        "        \"\"\"Extract key information from text for question generation.\"\"\"\n",
        "        if not self.nlp:\n",
        "            return {\"entities\": [], \"noun_chunks\": [], \"sentences\": []}\n",
        "\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract named entities\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'DATE', 'EVENT', 'WORK_OF_ART', 'CARDINAL', 'ORDINAL']:\n",
        "                entities.append({\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start': ent.start_char,\n",
        "                    'end': ent.end_char\n",
        "                })\n",
        "\n",
        "        # Extract noun chunks\n",
        "        noun_chunks = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) <= 4]\n",
        "\n",
        "        # Extract sentences\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.split()) > 5]\n",
        "\n",
        "        return {\n",
        "            \"entities\": entities,\n",
        "            \"noun_chunks\": noun_chunks,\n",
        "            \"sentences\": sentences\n",
        "        }\n",
        "\n",
        "    def generate_question_from_context(self, context: str, answer_text: str) -> str:\n",
        "        \"\"\"Generate a question given context and answer.\"\"\"\n",
        "        # Highlight the answer in the context for T5\n",
        "        highlighted_context = context.replace(answer_text, f\"<hl>{answer_text}<hl>\")\n",
        "        input_text = f\"generate question: {highlighted_context}\"\n",
        "\n",
        "        inputs = self.qg_tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.qg_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=64,\n",
        "                num_beams=4,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        question = self.qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return question\n",
        "\n",
        "    def generate_distractors_semantic(self, correct_answer: str, context: str, num_distractors: int = 3) -> List[str]:\n",
        "        \"\"\"Generate distractors using semantic similarity and context understanding.\"\"\"\n",
        "        distractors = []\n",
        "\n",
        "        # Method 1: Use fill-mask to generate contextually similar options\n",
        "        try:\n",
        "            # Replace answer with mask in context\n",
        "            masked_context = context.replace(correct_answer, \"<mask>\")\n",
        "            if \"<mask>\" in masked_context:\n",
        "                predictions = self.fill_mask(masked_context, top_k=20)\n",
        "                for pred in predictions:\n",
        "                    candidate = pred['token_str'].strip()\n",
        "                    if (candidate != correct_answer and\n",
        "                        candidate.lower() != correct_answer.lower() and\n",
        "                        len(candidate) > 1 and\n",
        "                        candidate not in distractors):\n",
        "                        distractors.append(candidate)\n",
        "                        if len(distractors) >= num_distractors:\n",
        "                            break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Method 2: Extract similar entities from context\n",
        "        if self.nlp and len(distractors) < num_distractors:\n",
        "            doc = self.nlp(context)\n",
        "            answer_doc = self.nlp(correct_answer)\n",
        "\n",
        "            # Get answer entity type\n",
        "            answer_label = None\n",
        "            for ent in answer_doc.ents:\n",
        "                answer_label = ent.label_\n",
        "                break\n",
        "\n",
        "            # Find similar entities\n",
        "            for ent in doc.ents:\n",
        "                if (ent.label_ == answer_label and\n",
        "                    ent.text != correct_answer and\n",
        "                    ent.text not in distractors):\n",
        "                    distractors.append(ent.text)\n",
        "                    if len(distractors) >= num_distractors:\n",
        "                        break\n",
        "\n",
        "        # Method 3: Generate using WordNet synonyms and related words\n",
        "        if len(distractors) < num_distractors:\n",
        "            try:\n",
        "                words = correct_answer.split()\n",
        "                for word in words:\n",
        "                    synsets = wordnet.synsets(word)\n",
        "                    for synset in synsets[:3]:\n",
        "                        for lemma in synset.lemmas()[:2]:\n",
        "                            candidate = lemma.name().replace('_', ' ')\n",
        "                            if (candidate != correct_answer and\n",
        "                                candidate.lower() != correct_answer.lower() and\n",
        "                                candidate not in distractors):\n",
        "                                distractors.append(candidate)\n",
        "                                if len(distractors) >= num_distractors:\n",
        "                                    break\n",
        "                        if len(distractors) >= num_distractors:\n",
        "                            break\n",
        "                    if len(distractors) >= num_distractors:\n",
        "                        break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Method 4: Generate plausible distractors based on answer type\n",
        "        if len(distractors) < num_distractors:\n",
        "            distractors.extend(self.generate_type_based_distractors(correct_answer, context))\n",
        "\n",
        "        # Remove duplicates and return\n",
        "        unique_distractors = []\n",
        "        seen = set()\n",
        "        for d in distractors:\n",
        "            if d.lower() not in seen and d.lower() != correct_answer.lower():\n",
        "                seen.add(d.lower())\n",
        "                unique_distractors.append(d)\n",
        "\n",
        "        return unique_distractors[:num_distractors]\n",
        "\n",
        "    def generate_type_based_distractors(self, correct_answer: str, context: str) -> List[str]:\n",
        "        \"\"\"Generate distractors based on answer type patterns.\"\"\"\n",
        "        distractors = []\n",
        "\n",
        "        # Check if answer is a number\n",
        "        if re.match(r'^\\d+$', correct_answer):\n",
        "            base_num = int(correct_answer)\n",
        "            variations = [\n",
        "                str(base_num + random.randint(1, 10)),\n",
        "                str(base_num - random.randint(1, 10)),\n",
        "                str(base_num * 2),\n",
        "                str(base_num // 2) if base_num > 1 else str(base_num + 1)\n",
        "            ]\n",
        "            distractors.extend([v for v in variations if v != correct_answer])\n",
        "\n",
        "        # Check if answer is a year\n",
        "        elif re.match(r'^\\d{4}$', correct_answer):\n",
        "            year = int(correct_answer)\n",
        "            year_variations = [\n",
        "                str(year + random.randint(1, 20)),\n",
        "                str(year - random.randint(1, 20)),\n",
        "                str(year + random.randint(50, 100)),\n",
        "                str(year - random.randint(50, 100))\n",
        "            ]\n",
        "            distractors.extend([y for y in year_variations if y != correct_answer])\n",
        "\n",
        "        # Check if answer is a percentage\n",
        "        elif '%' in correct_answer:\n",
        "            try:\n",
        "                num = float(correct_answer.replace('%', ''))\n",
        "                percent_variations = [\n",
        "                    f\"{num + random.randint(5, 25)}%\",\n",
        "                    f\"{num - random.randint(5, 25)}%\",\n",
        "                    f\"{num * 2}%\" if num <= 50 else f\"{num / 2}%\"\n",
        "                ]\n",
        "                distractors.extend([p for p in percent_variations if p != correct_answer])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return distractors[:3]\n",
        "\n",
        "    def validate_mcq_quality(self, question: str, correct_answer: str, distractors: List[str], context: str) -> Dict:\n",
        "        \"\"\"Validate the quality of generated MCQ.\"\"\"\n",
        "        # Check if the question can be answered correctly\n",
        "        try:\n",
        "            qa_result = self.qa_pipeline(question=question, context=context)\n",
        "            predicted_answer = qa_result['answer']\n",
        "            confidence = qa_result['score']\n",
        "\n",
        "            # Check if predicted answer matches or is similar to correct answer\n",
        "            similarity_threshold = 0.7\n",
        "            correct_embedding = self.sentence_model.encode([correct_answer])\n",
        "            predicted_embedding = self.sentence_model.encode([predicted_answer])\n",
        "            similarity = cosine_similarity(correct_embedding, predicted_embedding)[0][0]\n",
        "\n",
        "            is_answerable = similarity > similarity_threshold or correct_answer.lower() in predicted_answer.lower()\n",
        "\n",
        "        except:\n",
        "            is_answerable = False\n",
        "            confidence = 0.0\n",
        "            similarity = 0.0\n",
        "\n",
        "        # Check distractor quality\n",
        "        if len(distractors) > 0:\n",
        "            distractor_embeddings = self.sentence_model.encode(distractors)\n",
        "            correct_embedding = self.sentence_model.encode([correct_answer])\n",
        "\n",
        "            # Calculate similarity between distractors and correct answer\n",
        "            similarities = cosine_similarity(correct_embedding, distractor_embeddings)[0]\n",
        "            avg_distractor_similarity = np.mean(similarities)\n",
        "\n",
        "            # Good distractors should be somewhat similar but not too similar\n",
        "            distractor_quality = \"good\" if 0.3 < avg_distractor_similarity < 0.8 else \"poor\"\n",
        "        else:\n",
        "            distractor_quality = \"poor\"\n",
        "            avg_distractor_similarity = 0.0\n",
        "\n",
        "        return {\n",
        "            \"is_answerable\": is_answerable,\n",
        "            \"confidence\": confidence,\n",
        "            \"answer_similarity\": similarity,\n",
        "            \"distractor_quality\": distractor_quality,\n",
        "            \"avg_distractor_similarity\": avg_distractor_similarity\n",
        "        }\n",
        "\n",
        "    def generate_mcq(self, context: str, num_questions: int = 5) -> List[Dict]:\n",
        "        \"\"\"Generate multiple choice questions from context.\"\"\"\n",
        "        mcqs = []\n",
        "\n",
        "        # Extract key information\n",
        "        key_info = self.extract_key_information(context)\n",
        "\n",
        "        # Generate questions from entities\n",
        "        for entity in key_info[\"entities\"][:num_questions]:\n",
        "            correct_answer = entity[\"text\"]\n",
        "\n",
        "            # Generate question\n",
        "            question = self.generate_question_from_context(context, correct_answer)\n",
        "\n",
        "            # Generate distractors\n",
        "            distractors = self.generate_distractors_semantic(correct_answer, context, 3)\n",
        "\n",
        "            # Skip if not enough distractors\n",
        "            if len(distractors) < 2:\n",
        "                continue\n",
        "\n",
        "            # Validate quality\n",
        "            quality = self.validate_mcq_quality(question, correct_answer, distractors, context)\n",
        "\n",
        "            # Create options and shuffle\n",
        "            options = [correct_answer] + distractors[:3]\n",
        "            random.shuffle(options)\n",
        "            correct_option = chr(65 + options.index(correct_answer))  # A, B, C, D\n",
        "\n",
        "            mcq = {\n",
        "                \"question\": question,\n",
        "                \"options\": {\n",
        "                    \"A\": options[0],\n",
        "                    \"B\": options[1],\n",
        "                    \"C\": options[2] if len(options) > 2 else \"None of the above\",\n",
        "                    \"D\": options[3] if len(options) > 3 else \"All of the above\"\n",
        "                },\n",
        "                \"correct_answer\": correct_option,\n",
        "                \"correct_text\": correct_answer,\n",
        "                \"entity_type\": entity[\"label\"],\n",
        "                \"quality_score\": quality[\"confidence\"],\n",
        "                \"is_answerable\": quality[\"is_answerable\"]\n",
        "            }\n",
        "\n",
        "            # Only include high-quality MCQs\n",
        "            if quality[\"is_answerable\"] and quality[\"confidence\"] > 0.3:\n",
        "                mcqs.append(mcq)\n",
        "\n",
        "        # Generate additional questions from noun chunks if needed\n",
        "        if len(mcqs) < num_questions:\n",
        "            for chunk in key_info[\"noun_chunks\"][:num_questions - len(mcqs)]:\n",
        "                question = self.generate_question_from_context(context, chunk)\n",
        "                distractors = self.generate_distractors_semantic(chunk, context, 3)\n",
        "\n",
        "                if len(distractors) >= 2:\n",
        "                    quality = self.validate_mcq_quality(question, chunk, distractors, context)\n",
        "\n",
        "                    if quality[\"is_answerable\"] and quality[\"confidence\"] > 0.2:\n",
        "                        options = [chunk] + distractors[:3]\n",
        "                        random.shuffle(options)\n",
        "                        correct_option = chr(65 + options.index(chunk))\n",
        "\n",
        "                        mcq = {\n",
        "                            \"question\": question,\n",
        "                            \"options\": {\n",
        "                                \"A\": options[0],\n",
        "                                \"B\": options[1],\n",
        "                                \"C\": options[2] if len(options) > 2 else \"None of the above\",\n",
        "                                \"D\": options[3] if len(options) > 3 else \"All of the above\"\n",
        "                            },\n",
        "                            \"correct_answer\": correct_option,\n",
        "                            \"correct_text\": chunk,\n",
        "                            \"entity_type\": \"NOUN_CHUNK\",\n",
        "                            \"quality_score\": quality[\"confidence\"],\n",
        "                            \"is_answerable\": quality[\"is_answerable\"]\n",
        "                        }\n",
        "                        mcqs.append(mcq)\n",
        "\n",
        "        # Sort by quality score and return\n",
        "        mcqs.sort(key=lambda x: x[\"quality_score\"], reverse=True)\n",
        "        return mcqs[:num_questions]\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the MCQ generator.\"\"\"\n",
        "    generator = MultipleChoiceQuestionGenerator()\n",
        "\n",
        "    # Sample context\n",
        "    sample_context = \"\"\"\n",
        "    The Renaissance was a period of cultural, artistic, political and economic rebirth following the Middle Ages.\n",
        "    It began in Italy in the 14th century and spread throughout Europe. Leonardo da Vinci, born in 1452, was one\n",
        "    of the most famous Renaissance artists and inventors. He created masterpieces like the Mona Lisa and The Last Supper.\n",
        "    Michelangelo, another renowned artist, painted the ceiling of the Sistine Chapel between 1508 and 1512.\n",
        "    The Renaissance emphasized humanism, scientific inquiry, and artistic innovation. The printing press,\n",
        "    invented by Johannes Gutenberg around 1440, helped spread Renaissance ideas across Europe.\n",
        "    This period lasted approximately 300 years, from the 14th to the 17th century.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Multiple Choice Question Generator\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Get user input\n",
        "    user_context = input(\"Enter your context (or press Enter to use sample): \").strip()\n",
        "    if not user_context:\n",
        "        user_context = sample_context\n",
        "        print(\"Using sample context about the Renaissance...\")\n",
        "\n",
        "    try:\n",
        "        num_questions = int(input(\"Number of MCQs to generate (default 5): \") or \"5\")\n",
        "    except ValueError:\n",
        "        num_questions = 5\n",
        "\n",
        "    print(f\"\\nGenerating {num_questions} multiple choice questions...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Generate MCQs\n",
        "    mcqs = generator.generate_mcq(user_context, num_questions)\n",
        "\n",
        "    # Display results\n",
        "    if mcqs:\n",
        "        for i, mcq in enumerate(mcqs, 1):\n",
        "            print(f\"\\nQuestion {i}: \")\n",
        "            print(f\"Q: {mcq['question']}\")\n",
        "            print()\n",
        "            for option, text in mcq['options'].items():\n",
        "                print(f\"{option}) {text}\")\n",
        "            print(f\"\\nCorrect Answer: {mcq['correct_answer']}) {mcq['correct_text']}\")\n",
        "    else:\n",
        "        print(\"No high-quality MCQs could be generated from the provided context.\")\n",
        "        print(\"Try providing a longer, more detailed context with specific facts and entities.\")\n",
        "\n",
        "    print(\"\\nGeneration complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}