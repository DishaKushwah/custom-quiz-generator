{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNlLgN36uc2PRyXWiLUUS03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DishaKushwah/custom-quiz-generator/blob/main/mcq_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MCQS\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel\n",
        ")\n",
        "from transformers.pipelines import pipeline\n",
        "from transformers.models.t5 import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "class MultipleChoiceQuestionGenerator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the MCQ generator with advanced models.\"\"\"\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Load T5 model for question generation\n",
        "        self.qg_model_name = \"valhalla/t5-base-qg-hl\"\n",
        "        self.qg_tokenizer = T5Tokenizer.from_pretrained(self.qg_model_name)\n",
        "        self.qg_model = T5ForConditionalGeneration.from_pretrained(self.qg_model_name).to(self.device)\n",
        "\n",
        "        # Load question-answering pipeline for answer validation\n",
        "        self.qa_pipeline = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=\"deepset/roberta-large-squad2\",\n",
        "            tokenizer=\"deepset/roberta-large-squad2\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "        # Load sentence transformer for semantic similarity (distractor generation)\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Load spaCy for NLP processing\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = None\n",
        "\n",
        "        # Load fill-mask pipeline for generating distractors\n",
        "        self.fill_mask = pipeline(\"fill-mask\",model=\"roberta-large\",tokenizer=\"roberta-large\",device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "        # Download NLTK data\n",
        "        try:\n",
        "            nltk.download('wordnet', quiet=True)\n",
        "            nltk.download('omw-1.4', quiet=True)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def extract_key_information(self, text: str) -> Dict:\n",
        "        \"\"\"Extract key information from text for question generation.\"\"\"\n",
        "        if not self.nlp:\n",
        "            return {\"entities\": [], \"noun_chunks\": [], \"sentences\": []}\n",
        "\n",
        "        doc = self.nlp(text)\n",
        "        # Extract named entities\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'DATE', 'EVENT', 'WORK_OF_ART', 'CARDINAL', 'ORDINAL']:\n",
        "                entities.append({'text': ent.text,'label': ent.label_,'start': ent.start_char,'end': ent.end_char})\n",
        "\n",
        "        # Extract noun chunks\n",
        "        noun_chunks = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) <= 4]\n",
        "\n",
        "        # Extract sentences\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.split()) > 5]\n",
        "\n",
        "        return {\"entities\": entities,\"noun_chunks\": noun_chunks,\"sentences\": sentences}\n",
        "\n",
        "    def generate_question_from_context(self, context: str, answer_text: str) -> str:\n",
        "        \"\"\"Generate a question given context and answer.\"\"\"\n",
        "        # Highlight the answer in the context for T5\n",
        "        highlighted_context = context.replace(answer_text, f\"<hl>{answer_text}<hl>\")\n",
        "        input_text = f\"generate question: {highlighted_context}\"\n",
        "        inputs = self.qg_tokenizer.encode_plus(input_text,max_length=512,truncation=True,padding=True,return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.qg_model.generate(inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"],max_length=64,num_beams=4,temperature=0.8,do_sample=True,early_stopping=True)\n",
        "\n",
        "        question = self.qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return question\n",
        "\n",
        "    def generate_distractors_semantic(self, correct_answer: str, context: str, num_distractors: int = 3) -> List[str]:\n",
        "        \"\"\"Generate distractors using semantic similarity and context understanding.\"\"\"\n",
        "        distractors = []\n",
        "\n",
        "        # Method 1: Use fill-mask to generate contextually similar options\n",
        "        try:\n",
        "            # Replace answer with mask in context\n",
        "            masked_context = context.replace(correct_answer, \"<mask>\")\n",
        "            if \"<mask>\" in masked_context:\n",
        "                predictions = self.fill_mask(masked_context, top_k=20)\n",
        "                for pred in predictions:\n",
        "                    candidate = pred['token_str'].strip()\n",
        "                    if (candidate != correct_answer and\n",
        "                        candidate.lower() != correct_answer.lower() and\n",
        "                        len(candidate) > 1 and\n",
        "                        candidate not in distractors):\n",
        "                        distractors.append(candidate)\n",
        "                        if len(distractors) >= num_distractors:\n",
        "                            break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Method 2: Extract similar entities from context\n",
        "        if self.nlp and len(distractors) < num_distractors:\n",
        "            doc = self.nlp(context)\n",
        "            answer_doc = self.nlp(correct_answer)\n",
        "\n",
        "            # Get answer entity type\n",
        "            answer_label = None\n",
        "            for ent in answer_doc.ents:\n",
        "                answer_label = ent.label_\n",
        "                break\n",
        "\n",
        "            # Find similar entities\n",
        "            for ent in doc.ents:\n",
        "                if (ent.label_ == answer_label and ent.text != correct_answer and ent.text not in distractors):\n",
        "                    distractors.append(ent.text)\n",
        "                    if len(distractors) >= num_distractors:\n",
        "                        break\n",
        "\n",
        "        # Method 3: Generate using WordNet synonyms and related words\n",
        "        if len(distractors) < num_distractors:\n",
        "            try:\n",
        "                words = correct_answer.split()\n",
        "                for word in words:\n",
        "                    synsets = wordnet.synsets(word)\n",
        "                    for synset in synsets[:3]:\n",
        "                        for lemma in synset.lemmas()[:2]:\n",
        "                            candidate = lemma.name().replace('_', ' ')\n",
        "                            if (candidate != correct_answer and\n",
        "                                candidate.lower() != correct_answer.lower() and\n",
        "                                candidate not in distractors):\n",
        "                                distractors.append(candidate)\n",
        "                                if len(distractors) >= num_distractors:\n",
        "                                    break\n",
        "                        if len(distractors) >= num_distractors:\n",
        "                            break\n",
        "                    if len(distractors) >= num_distractors:\n",
        "                        break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Method 4: Generate plausible distractors based on answer type\n",
        "        if len(distractors) < num_distractors:\n",
        "            distractors.extend(self.generate_type_based_distractors(correct_answer, context))\n",
        "\n",
        "        # Remove duplicates and return\n",
        "        unique_distractors = []\n",
        "        seen = set()\n",
        "        for d in distractors:\n",
        "            if d.lower() not in seen and d.lower() != correct_answer.lower():\n",
        "                seen.add(d.lower())\n",
        "                unique_distractors.append(d)\n",
        "        return unique_distractors[:num_distractors]\n",
        "\n",
        "    def validate_mcq_quality(self, question: str, correct_answer: str, distractors: List[str], context: str) -> Dict:\n",
        "        \"\"\"Validate the quality of generated MCQ.\"\"\"\n",
        "        # Check if the question can be answered correctly\n",
        "        try:\n",
        "            qa_result = self.qa_pipeline(question=question, context=context)\n",
        "            predicted_answer = qa_result['answer']\n",
        "            confidence = qa_result['score']\n",
        "\n",
        "            # Check if predicted answer matches or is similar to correct answer\n",
        "            similarity_threshold = 0.7\n",
        "            correct_embedding = self.sentence_model.encode([correct_answer])\n",
        "            predicted_embedding = self.sentence_model.encode([predicted_answer])\n",
        "            similarity = cosine_similarity(correct_embedding, predicted_embedding)[0][0]\n",
        "            is_answerable = similarity > similarity_threshold or correct_answer.lower() in predicted_answer.lower()\n",
        "\n",
        "        except:\n",
        "            is_answerable = False\n",
        "            confidence = 0.0\n",
        "            similarity = 0.0\n",
        "\n",
        "        # Check distractor quality\n",
        "        if len(distractors) > 0:\n",
        "            distractor_embeddings = self.sentence_model.encode(distractors)\n",
        "            correct_embedding = self.sentence_model.encode([correct_answer])\n",
        "\n",
        "            # Calculate similarity between distractors and correct answer\n",
        "            similarities = cosine_similarity(correct_embedding, distractor_embeddings)[0]\n",
        "            avg_distractor_similarity = np.mean(similarities)\n",
        "\n",
        "            # Good distractors should be somewhat similar but not too similar\n",
        "            distractor_quality = \"good\" if 0.3 < avg_distractor_similarity < 0.8 else \"poor\"\n",
        "        else:\n",
        "            distractor_quality = \"poor\"\n",
        "            avg_distractor_similarity = 0.0\n",
        "\n",
        "        return {\"is_answerable\": is_answerable,\"confidence\": confidence,\"answer_similarity\": similarity,\"distractor_quality\": distractor_quality,\"avg_distractor_similarity\": avg_distractor_similarity }\n",
        "\n",
        "    def generate_mcq(self, context: str, num_questions: int = 5) -> List[Dict]:\n",
        "        \"\"\"Generate multiple choice questions from context.\"\"\"\n",
        "        mcqs = []\n",
        "\n",
        "        # Extract key information\n",
        "        key_info = self.extract_key_information(context)\n",
        "\n",
        "        # Generate questions from entities\n",
        "        for entity in key_info[\"entities\"][:num_questions]:\n",
        "            correct_answer = entity[\"text\"]\n",
        "\n",
        "            # Generate question\n",
        "            question = self.generate_question_from_context(context, correct_answer)\n",
        "\n",
        "            # Generate distractors\n",
        "            distractors = self.generate_distractors_semantic(correct_answer, context, 3)\n",
        "\n",
        "            # Skip if not enough distractors\n",
        "            if len(distractors) < 2:\n",
        "                continue\n",
        "\n",
        "            # Validate quality\n",
        "            quality = self.validate_mcq_quality(question, correct_answer, distractors, context)\n",
        "\n",
        "            # Create options and shuffle\n",
        "            options = [correct_answer] + distractors[:3]\n",
        "            random.shuffle(options)\n",
        "            correct_option = chr(65 + options.index(correct_answer))  # A, B, C, D\n",
        "\n",
        "            mcq = {\n",
        "                \"question\": question,\n",
        "                \"options\": {\"A\": options[0],\"B\": options[1],\"C\": options[2] if len(options) > 2 else \"None of the above\",\"D\": options[3] if len(options) > 3 else \"All of the above\"},\n",
        "                \"correct_answer\": correct_option,\n",
        "                \"correct_text\": correct_answer,\n",
        "                \"entity_type\": entity[\"label\"],\n",
        "                \"quality_score\": quality[\"confidence\"],\n",
        "                \"is_answerable\": quality[\"is_answerable\"]\n",
        "            }\n",
        "\n",
        "            # Only include high-quality MCQs\n",
        "            if quality[\"is_answerable\"] and quality[\"confidence\"] > 0.3:\n",
        "                mcqs.append(mcq)\n",
        "\n",
        "        # Generate additional questions from noun chunks if needed\n",
        "        if len(mcqs) < num_questions:\n",
        "            for chunk in key_info[\"noun_chunks\"][:num_questions - len(mcqs)]:\n",
        "                question = self.generate_question_from_context(context, chunk)\n",
        "                distractors = self.generate_distractors_semantic(chunk, context, 3)\n",
        "\n",
        "                if len(distractors) >= 2:\n",
        "                    quality = self.validate_mcq_quality(question, chunk, distractors, context)\n",
        "\n",
        "                    if quality[\"is_answerable\"] and quality[\"confidence\"] > 0.2:\n",
        "                        options = [chunk] + distractors[:3]\n",
        "                        random.shuffle(options)\n",
        "                        correct_option = chr(65 + options.index(chunk))\n",
        "\n",
        "                        mcq = {\n",
        "                            \"question\": question,\n",
        "                            \"options\": {\"A\": options[0],\"B\": options[1],\"C\": options[2] if len(options) > 2 else \"None of the above\",\"D\": options[3] if len(options) > 3 else \"All of the above\"},\n",
        "                            \"correct_answer\": correct_option,\n",
        "                            \"correct_text\": chunk,\n",
        "                            \"entity_type\": \"NOUN_CHUNK\",\n",
        "                            \"quality_score\": quality[\"confidence\"],\n",
        "                            \"is_answerable\": quality[\"is_answerable\"]\n",
        "                        }\n",
        "                        mcqs.append(mcq)\n",
        "\n",
        "        # Sort by quality score and return\n",
        "        mcqs.sort(key=lambda x: x[\"quality_score\"], reverse=True)\n",
        "        return mcqs[:num_questions]\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the MCQ generator.\"\"\"\n",
        "    generator = MultipleChoiceQuestionGenerator()\n",
        "    print(\"Multiple Choice Question Generator\")\n",
        "\n",
        "    # Get user input\n",
        "    user_context = input(\"Enter your context: \").strip()\n",
        "    try:\n",
        "        num_questions = int(input(\"Number of MCQs to generate (default 5): \") or \"5\")\n",
        "    except ValueError:\n",
        "        num_questions = 5\n",
        "    print(f\"\\nGenerating {num_questions} multiple choice questions...\")\n",
        "\n",
        "    # Generate MCQs\n",
        "    mcqs = generator.generate_mcq(user_context, num_questions)\n",
        "    # Display results\n",
        "    if mcqs:\n",
        "        for i, mcq in enumerate(mcqs, 1):\n",
        "            print(f\"\\nQuestion {i}: \")\n",
        "            print(f\"Q: {mcq['question']}\")\n",
        "            print()\n",
        "            for option, text in mcq['options'].items():\n",
        "                print(f\"{option}) {text}\")\n",
        "            print(f\"\\nCorrect Answer: {mcq['correct_answer']}) {mcq['correct_text']}\")\n",
        "    else:\n",
        "        print(\"No high-quality MCQs could be generated from the provided context.\")\n",
        "        print(\"Try providing a longer, more detailed context with specific facts and entities.\")\n",
        "\n",
        "    print(\"\\nGeneration complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1ic84jCGc-u",
        "outputId": "82f5601f-0e8f-4ca7-d9a4-b514a58df793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiple Choice Question Generator\n",
            "Enter your context (or press Enter to use sample): India has numerous national parks dedicated to preserving wildlife and biodiversity. Some of the most famous include Jim Corbett National Park in Uttarakhand, known for tigers; Kaziranga National Park in Assam, home to the one-horned rhinoceros; and Sundarbans in West Bengal, famous for mangrove forests and Royal Bengal Tigers. These parks also support eco-tourism and help protect endangered species and fragile ecosystems.\n",
            "Number of MCQs to generate (default 5): 4\n",
            "\n",
            "Generating 4 multiple choice questions...\n",
            "\n",
            "Question 1: \n",
            "Q: What country has numerous national parks dedicated to preserving wildlife and biodiversity?\n",
            "\n",
            "A) Asia\n",
            "B) India\n",
            "C) Indian\n",
            "D) Pakistan\n",
            "\n",
            "Correct Answer: B) India\n",
            "\n",
            "Question 2: \n",
            "Q: What national park in Uttarakhand is known for tigers?\n",
            "\n",
            "A) Sanctuary\n",
            "B) Leh\n",
            "C) Jim Corbett National Park\n",
            "D) Gir\n",
            "\n",
            "Correct Answer: C) Jim Corbett National Park\n",
            "\n",
            "Question 3: \n",
            "Q: Where is Jim Corbett National Park located?\n",
            "\n",
            "A) Uttarakhand\n",
            "B) Maharashtra\n",
            "C) Kerala\n",
            "D) Bengal\n",
            "\n",
            "Correct Answer: A) Uttarakhand\n",
            "\n",
            "Question 4: \n",
            "Q: How many horned rhinoceros live in Kaziranga National Park?\n",
            "\n",
            "A) big\n",
            "B) one\n",
            "C) great\n",
            "D) two\n",
            "\n",
            "Correct Answer: B) one\n",
            "\n",
            "Generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cR-tHf45INso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}